services:
  namenode:
    image: razer99/hadoop-cluster-mouin-boubakri:latest
    container_name: namenode
    hostname: namenode
    environment:
      - HADOOP_ROLE=namenode
      - CLUSTER_NAME=gaza-youtube-analytics
    ports:
      - "9870:9870"
      - "9000:9000"
      - "8088:8088"
    volumes:
      - namenode_data:/hadoop/dfs/name
      - ./hadoop-configs/core-site.xml:/usr/local/hadoop/etc/hadoop/core-site.xml:ro
      - ./hadoop-configs/hdfs-site.xml:/usr/local/hadoop/etc/hadoop/hdfs-site.xml:ro
      - ./hadoop-configs/mapred-site.xml:/usr/local/hadoop/etc/hadoop/mapred-site.xml:ro
      - ./hadoop-configs/yarn-site.xml:/usr/local/hadoop/etc/hadoop/yarn-site.xml:ro
      - ./start-hadoop.sh:/start-hadoop.sh:ro
      - ./pyspark_gaza.py:/opt/pyspark_gaza.py:ro
    networks:
      - hadoop-network
    healthcheck:
      test: ["CMD", "bash", "-c", "jps | grep -q NameNode"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    command: ["/bin/bash", "/start-hadoop.sh"]

  datanode1:
    image: razer99/hadoop-cluster-mouin-boubakri:latest
    container_name: datanode1
    hostname: datanode1
    environment:
      - HADOOP_ROLE=datanode
      - CLUSTER_NAME=gaza-youtube-analytics
    ports:
      - "9864:9864"
    volumes:
      - datanode1_data:/hadoop/dfs/data
      - ./hadoop-configs/core-site.xml:/usr/local/hadoop/etc/hadoop/core-site.xml:ro
      - ./hadoop-configs/hdfs-site.xml:/usr/local/hadoop/etc/hadoop/hdfs-site.xml:ro
      - ./hadoop-configs/mapred-site.xml:/usr/local/hadoop/etc/hadoop/mapred-site.xml:ro
      - ./hadoop-configs/yarn-site.xml:/usr/local/hadoop/etc/hadoop/yarn-site.xml:ro
      - ./start-hadoop.sh:/start-hadoop.sh:ro
    networks:
      - hadoop-network
    depends_on:
      namenode:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "bash", "-c", "jps | grep -q DataNode"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 45s
    command: ["/bin/bash", "/start-hadoop.sh"]

  datanode2:
    image: razer99/hadoop-cluster-mouin-boubakri:latest
    container_name: datanode2
    hostname: datanode2
    environment:
      - HADOOP_ROLE=datanode
      - CLUSTER_NAME=gaza-youtube-analytics
    ports:
      - "9865:9864"
    volumes:
      - datanode2_data:/hadoop/dfs/data
      - ./hadoop-configs/core-site.xml:/usr/local/hadoop/etc/hadoop/core-site.xml:ro
      - ./hadoop-configs/hdfs-site.xml:/usr/local/hadoop/etc/hadoop/hdfs-site.xml:ro
      - ./hadoop-configs/mapred-site.xml:/usr/local/hadoop/etc/hadoop/mapred-site.xml:ro
      - ./hadoop-configs/yarn-site.xml:/usr/local/hadoop/etc/hadoop/yarn-site.xml:ro
      - ./start-hadoop.sh:/start-hadoop.sh:ro
    networks:
      - hadoop-network
    depends_on:
      namenode:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "bash", "-c", "jps | grep -q DataNode"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 45s
    command: ["/bin/bash", "/start-hadoop.sh"]

  datanode3:
    image: razer99/hadoop-cluster-mouin-boubakri:latest
    container_name: datanode3
    hostname: datanode3
    environment:
      - HADOOP_ROLE=datanode
      - CLUSTER_NAME=gaza-youtube-analytics
    ports:
      - "9866:9864"
    volumes:
      - datanode3_data:/hadoop/dfs/data
      - ./hadoop-configs/core-site.xml:/usr/local/hadoop/etc/hadoop/core-site.xml:ro
      - ./hadoop-configs/hdfs-site.xml:/usr/local/hadoop/etc/hadoop/hdfs-site.xml:ro
      - ./hadoop-configs/mapred-site.xml:/usr/local/hadoop/etc/hadoop/mapred-site.xml:ro
      - ./hadoop-configs/yarn-site.xml:/usr/local/hadoop/etc/hadoop/yarn-site.xml:ro
      - ./start-hadoop.sh:/start-hadoop.sh:ro
    networks:
      - hadoop-network
    depends_on:
      namenode:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "bash", "-c", "jps | grep -q DataNode"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 45s
    command: ["/bin/bash", "/start-hadoop.sh"]

  datanode4:
    image: razer99/hadoop-cluster-mouin-boubakri:latest
    container_name: datanode4
    hostname: datanode4
    environment:
      - HADOOP_ROLE=datanode
      - CLUSTER_NAME=gaza-youtube-analytics
    ports:
      - "9867:9864"
    volumes:
      - datanode4_data:/hadoop/dfs/data
      - ./hadoop-configs/core-site.xml:/usr/local/hadoop/etc/hadoop/core-site.xml:ro
      - ./hadoop-configs/hdfs-site.xml:/usr/local/hadoop/etc/hadoop/hdfs-site.xml:ro
      - ./hadoop-configs/mapred-site.xml:/usr/local/hadoop/etc/hadoop/mapred-site.xml:ro
      - ./hadoop-configs/yarn-site.xml:/usr/local/hadoop/etc/hadoop/yarn-site.xml:ro
      - ./start-hadoop.sh:/start-hadoop.sh:ro
    networks:
      - hadoop-network
    depends_on:
      namenode:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "bash", "-c", "jps | grep -q DataNode"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 45s
    command: ["/bin/bash", "/start-hadoop.sh"]

  secondarynamenode:
    image: razer99/hadoop-cluster-mouin-boubakri:latest
    container_name: secondarynamenode
    hostname: secondarynamenode
    environment:
      - HADOOP_ROLE=secondarynamenode
      - CLUSTER_NAME=gaza-youtube-analytics
    ports:
      - "9868:9868"
    volumes:
      - secondarynamenode_data:/hadoop/dfs/namesecondary
      - ./hadoop-configs/core-site.xml:/usr/local/hadoop/etc/hadoop/core-site.xml:ro
      - ./hadoop-configs/hdfs-site.xml:/usr/local/hadoop/etc/hadoop/hdfs-site.xml:ro
      - ./hadoop-configs/mapred-site.xml:/usr/local/hadoop/etc/hadoop/mapred-site.xml:ro
      - ./hadoop-configs/yarn-site.xml:/usr/local/hadoop/etc/hadoop/yarn-site.xml:ro
      - ./start-hadoop.sh:/start-hadoop.sh:ro
    networks:
      - hadoop-network
    depends_on:
      namenode:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "bash", "-c", "jps | grep -q SecondaryNameNode"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 45s
    command: ["/bin/bash", "/start-hadoop.sh"]

  spark-master:
    image: apache/spark-py:v3.4.0
    container_name: spark-master
    hostname: spark-master
    user: root
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - ./pyspark_gaza.py:/opt/pyspark_gaza.py:ro
      - spark_master_data:/opt/spark-data
    networks:
      - hadoop-network
    healthcheck:
      test: ["CMD", "bash", "-c", "pgrep -f org.apache.spark.deploy.master.Master || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    entrypoint: ["/opt/spark/bin/spark-class"]
    command: ["org.apache.spark.deploy.master.Master"]

  spark-worker-1:
    image: apache/spark-py:v3.4.0
    container_name: spark-worker-1
    hostname: spark-worker-1
    user: root
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=4G
      - SPARK_WORKER_CORES=2
    ports:
      - "8081:8081"
    volumes:
      - spark_worker1_data:/opt/spark-data
    networks:
      - hadoop-network
    depends_on:
      spark-master:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "bash", "-c", "pgrep -f org.apache.spark.deploy.worker.Worker || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    entrypoint: ["/opt/spark/bin/spark-class"]
    command: ["org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077"]

  spark-worker-2:
    image: apache/spark-py:v3.4.0
    container_name: spark-worker-2
    hostname: spark-worker-2
    user: root
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=4G
      - SPARK_WORKER_CORES=2
    ports:
      - "8082:8081"
    volumes:
      - spark_worker2_data:/opt/spark-data
    networks:
      - hadoop-network
    depends_on:
      spark-master:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "bash", "-c", "pgrep -f org.apache.spark.deploy.worker.Worker || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    entrypoint: ["/opt/spark/bin/spark-class"]
    command: ["org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077"]

networks:
  hadoop-network:
    driver: bridge

volumes:
  namenode_data:
  secondarynamenode_data:
  datanode1_data:
  datanode2_data:
  datanode3_data:
  datanode4_data:
  spark_master_data:
  spark_worker1_data:
  spark_worker2_data:
